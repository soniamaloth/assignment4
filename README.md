
# Home Assignment 4 ‚Äì CS5720 Neural Networks and Deep Learning

University of Central Missouri  
Department of Computer Science & Cybersecurity

This repository contains the solutions for **Home Assignment 4** of the **CS5720 Neural Networks and Deep Learning** course. The assignment covers **Generative Adversarial Networks (GANs)**, **data poisoning**, **ethical considerations in AI**, and **fairness analysis**.


## üì¶ Setup Instructions

### 1Ô∏è‚É£ Clone Repository
```bash
git clone <repository-url>
cd Home-Assignment-4
````

### 2Ô∏è‚É£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Execution

```bash
# For GAN Implementation
python gan_mnist.py

# For Data Poisoning Simulation
python data_poisoning.py
```

---

## Questions Overview

### **Question 1: GAN Architecture**

**Objective**: Explain the architecture of a GAN, including:

* Roles of the **Generator** and **Discriminator**
* Adversarial process of training
* Competitive improvement

**Deliverables**:

* **Diagram**: `gan_architecture.png`

**Notes**:
The diagram illustrates multilayer neural networks with the Generator creating fake MNIST digits and the Discriminator distinguishing real vs. fake samples. Loss feedback loops are shown clearly.

---

### **Question 2: Ethics of AI and Potential Harm**

**Objective**: Analyze ethical implications of AI, focusing on misinformation in generative AI.

**Deliverable (Narration Text)**:

> "This analysis focuses on misinformation in generative AI as a real-world harm. A hypothetical application is the use of GANs to generate fake news articles, where a model trained on mixed authentic and fabricated data produces convincing but false stories about political events, leading to public confusion and manipulation. This can occur if the training data includes unverified sources. Two mitigation strategies from the lecture include implementing robust data validation processes to filter out unreliable inputs before training, ensuring only verified data is used, and deploying real-time fact-checking algorithms to flag and correct misinformation generated by the model, reducing its societal impact."

---

### **Question 3: Basic GAN Implementation**

**Objective**: Implement a GAN to generate MNIST digits using **PyTorch**, trained for 20 epochs.

**Deliverables**:

* **Generated Images**:

  * `output/epoch_0.png`
  * `output/epoch_10.png`
  * `output/epoch_19.png`
* **Loss Plot**: `output/loss_plot.png`

**Notes**:
The GAN processes **79 batches per epoch** on a **10,000-sample MNIST subset**. Over epochs, generated images evolve from noise to recognizable digits.

---

###  **Question 4: Data Poisoning Simulation**

**Objective**: Simulate a data poisoning attack on the **IMDB** dataset and compare clean vs. poisoned model performance.

**Deliverables**:

* **Confusion Matrices**: `confusion_matrices.png`

**Notes**:
4,876 (30% of 16,256) reviews containing "movie" had their labels flipped, demonstrating significant performance degradation due to poisoning.

---

###  **Question 5: Generative AI Legal and Ethical Considerations**

**Objective**: Investigate legal and ethical concerns of AI-generated content.

**Deliverable (Narration Text)**:

> "Generative AI raises significant legal and ethical concerns. One example is memorizing private data, as seen with GPT-2, where the model inadvertently retained names or personal details from training texts, violating privacy rights. Another issue is generating copyrighted material, like Harry Potter text, where GANs may reproduce protected content, leading to intellectual property disputes. I believe generative AI models should be restricted from certain data during training, such as private datasets or copyrighted works, to prevent these violations. Justification includes protecting individual privacy and respecting intellectual property laws, though this requires careful curation and legal frameworks to balance innovation with ethical standards."

---

###  **Question 6: Bias and Fairness in AI Systems**

**Objective**: Analyze bias in AI, propose detection and mitigation methods, and emphasize fairness importance.

**Deliverable**:

> "This analysis focuses on the false negative rate parity metric from the Aequitas Bias Audit Tool. This metric measures the equality of false negative rates across different groups, ensuring that the proportion of missed positive predictions is similar for all demographics, such as gender or race. It‚Äôs important because unequal false negatives can lead to unfair outcomes, like denying loans to qualified individuals from underrepresented groups. A model might fail this metric if its training data overrepresents one group, causing higher false negatives for others. Mitigation involves reweighting the dataset to balance representation or applying fairness constraints during training to align rates across groups."

---
